{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6arNfXz4J-l",
        "outputId": "253cda7f-8450-4f51-b21e-b9aece3a0608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q1 - Word Frequency Distribution (filtered):\n",
            "love: 1\n",
            "exploring: 1\n",
            "intersection: 1\n",
            "technology: 1\n",
            "human: 1\n",
            "behavior: 1\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "transforming: 1\n",
            "industries: 1\n",
            "especially: 1\n",
            "healthcare: 1\n",
            "education: 1\n",
            "fascination: 1\n",
            "lies: 1\n",
            "machines: 1\n",
            "learn: 1\n",
            "think: 1\n",
            "make: 1\n",
            "decisions: 1\n",
            "books: 1\n",
            "podcasts: 1\n",
            "keep: 1\n",
            "updated: 1\n",
            "cuttingedge: 1\n",
            "advancements: 1\n",
            "im: 1\n",
            "particularly: 1\n",
            "intrigued: 1\n",
            "ethical: 1\n",
            "implications: 1\n",
            "autonomous: 1\n",
            "systems: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# ASSIGNMENT 9 - NLP with Python\n",
        "# =============================\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# ----------------------------------------\n",
        "# Q1. Text Preprocessing\n",
        "# ----------------------------------------\n",
        "\n",
        "text = \"\"\"I love exploring the intersection of technology and human behavior.\n",
        "Artificial Intelligence is transforming industries, especially healthcare and education.\n",
        "My fascination lies in how machines learn to think and make decisions.\n",
        "Books and podcasts keep me updated on cutting-edge advancements.\n",
        "I'm particularly intrigued by ethical implications of autonomous systems.\"\"\"\n",
        "\n",
        "text_lower = text.lower()\n",
        "text_clean = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "word_tokens = word_tokenize(text_clean)\n",
        "sent_tokens = sent_tokenize(text_clean)\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_words = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "\n",
        "print(\"\\nQ1 - Word Frequency Distribution (filtered):\")\n",
        "fdist = FreqDist(filtered_words)\n",
        "for word, freq in fdist.most_common():\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# Q2. Stemming and Lemmatization\n",
        "# ----------------------------------------\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"\\nQ2 - Stemming Comparison:\")\n",
        "print(\"{:<15} {:<15} {:<15}\".format(\"Word\", \"Porter\", \"Lancaster\"))\n",
        "for word in filtered_words:\n",
        "    print(\"{:<15} {:<15} {:<15}\".format(word, porter.stem(word), lancaster.stem(word)))\n",
        "\n",
        "print(\"\\nQ2 - Lemmatization:\")\n",
        "for word in filtered_words:\n",
        "    print(f\"{word} → {lemmatizer.lemmatize(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdXN_k_S4ecp",
        "outputId": "5afe33c3-7d10-48c0-d726-6807d8d556e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q2 - Stemming Comparison:\n",
            "Word            Porter          Lancaster      \n",
            "love            love            lov            \n",
            "exploring       explor          expl           \n",
            "intersection    intersect       intersect      \n",
            "technology      technolog       technolog      \n",
            "human           human           hum            \n",
            "behavior        behavior        behavy         \n",
            "artificial      artifici        art            \n",
            "intelligence    intellig        intellig       \n",
            "transforming    transform       transform      \n",
            "industries      industri        industry       \n",
            "especially      especi          espec          \n",
            "healthcare      healthcar       healthc        \n",
            "education       educ            educ           \n",
            "fascination     fascin          fascin         \n",
            "lies            lie             lie            \n",
            "machines        machin          machin         \n",
            "learn           learn           learn          \n",
            "think           think           think          \n",
            "make            make            mak            \n",
            "decisions       decis           decid          \n",
            "books           book            book           \n",
            "podcasts        podcast         podcast        \n",
            "keep            keep            keep           \n",
            "updated         updat           upd            \n",
            "cuttingedge     cuttingedg      cuttingedg     \n",
            "advancements    advanc          adv            \n",
            "im              im              im             \n",
            "particularly    particularli    particul       \n",
            "intrigued       intrigu         intrigu        \n",
            "ethical         ethic           eth            \n",
            "implications    implic          imply          \n",
            "autonomous      autonom         autonom        \n",
            "systems         system          system         \n",
            "\n",
            "Q2 - Lemmatization:\n",
            "love → love\n",
            "exploring → exploring\n",
            "intersection → intersection\n",
            "technology → technology\n",
            "human → human\n",
            "behavior → behavior\n",
            "artificial → artificial\n",
            "intelligence → intelligence\n",
            "transforming → transforming\n",
            "industries → industry\n",
            "especially → especially\n",
            "healthcare → healthcare\n",
            "education → education\n",
            "fascination → fascination\n",
            "lies → lie\n",
            "machines → machine\n",
            "learn → learn\n",
            "think → think\n",
            "make → make\n",
            "decisions → decision\n",
            "books → book\n",
            "podcasts → podcasts\n",
            "keep → keep\n",
            "updated → updated\n",
            "cuttingedge → cuttingedge\n",
            "advancements → advancement\n",
            "im → im\n",
            "particularly → particularly\n",
            "intrigued → intrigued\n",
            "ethical → ethical\n",
            "implications → implication\n",
            "autonomous → autonomous\n",
            "systems → system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# Q3. Regular Expressions and Splitting\n",
        "# ----------------------------------------\n",
        "\n",
        "long_words = re.findall(r'\\b\\w{6,}\\b', text)\n",
        "print(\"\\nQ3 - Words > 5 letters:\", long_words)\n",
        "\n",
        "\n",
        "numbers = re.findall(r'\\b\\d+\\b', text)\n",
        "print(\"Q3 - Numbers:\", numbers)\n",
        "\n",
        "\n",
        "capitalized = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
        "print(\"Q3 - Capitalized Words:\", capitalized)\n",
        "\n",
        "\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "print(\"Q3 - Alphabet-only Words:\", alpha_words)\n",
        "\n",
        "\n",
        "vowel_words = re.findall(r'\\b[aeiouAEIOU]\\w*', text)\n",
        "print(\"Q3 - Words starting with vowels:\", vowel_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXcLe6iA4jxx",
        "outputId": "46cbe4b2-f181-4dc9-d915-a940f75d5d66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q3 - Words > 5 letters: ['exploring', 'intersection', 'technology', 'behavior', 'Artificial', 'Intelligence', 'transforming', 'industries', 'especially', 'healthcare', 'education', 'fascination', 'machines', 'decisions', 'podcasts', 'updated', 'cutting', 'advancements', 'particularly', 'intrigued', 'ethical', 'implications', 'autonomous', 'systems']\n",
            "Q3 - Numbers: []\n",
            "Q3 - Capitalized Words: ['I', 'Artificial', 'Intelligence', 'My', 'Books', 'I']\n",
            "Q3 - Alphabet-only Words: ['I', 'love', 'exploring', 'the', 'intersection', 'of', 'technology', 'and', 'human', 'behavior', 'Artificial', 'Intelligence', 'is', 'transforming', 'industries', 'especially', 'healthcare', 'and', 'education', 'My', 'fascination', 'lies', 'in', 'how', 'machines', 'learn', 'to', 'think', 'and', 'make', 'decisions', 'Books', 'and', 'podcasts', 'keep', 'me', 'updated', 'on', 'cutting', 'edge', 'advancements', 'I', 'm', 'particularly', 'intrigued', 'by', 'ethical', 'implications', 'of', 'autonomous', 'systems']\n",
            "Q3 - Words starting with vowels: ['I', 'exploring', 'intersection', 'of', 'and', 'Artificial', 'Intelligence', 'is', 'industries', 'especially', 'and', 'education', 'in', 'and', 'and', 'updated', 'on', 'edge', 'advancements', 'I', 'intrigued', 'ethical', 'implications', 'of', 'autonomous']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# Q4. Custom Tokenization and Regex Cleaning\n",
        "# ----------------------------------------\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    text = re.sub(r'[^\\w\\s\\-\\']', '', text)\n",
        "    return re.findall(r\"\\b(?:\\d+\\.\\d+|\\d+|\\w+(?:-\\w+)*|\\w+'\\w+)\\b\", text)\n",
        "\n",
        "custom_tokens = custom_tokenizer(text)\n",
        "print(\"\\nQ4 - Custom Tokens:\", custom_tokens)\n",
        "\n",
        "text_sub = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
        "text_sub = re.sub(r'http[s]?://\\S+|www\\.\\S+', '<URL>', text_sub)\n",
        "text_sub = re.sub(r'(\\+91\\s?\\d{10}|\\d{3}[-\\s]\\d{3}[-\\s]\\d{4})', '<PHONE>', text_sub)\n",
        "print(\"Q4 - Cleaned Text with Substitutions:\")\n",
        "print(text_sub)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kufq3RCA4ky2",
        "outputId": "83ea9fdf-60a7-4400-fab9-fb5fa769b0f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q4 - Custom Tokens: ['I', 'love', 'exploring', 'the', 'intersection', 'of', 'technology', 'and', 'human', 'behavior', 'Artificial', 'Intelligence', 'is', 'transforming', 'industries', 'especially', 'healthcare', 'and', 'education', 'My', 'fascination', 'lies', 'in', 'how', 'machines', 'learn', 'to', 'think', 'and', 'make', 'decisions', 'Books', 'and', 'podcasts', 'keep', 'me', 'updated', 'on', 'cutting-edge', 'advancements', 'I', 'm', 'particularly', 'intrigued', 'by', 'ethical', 'implications', 'of', 'autonomous', 'systems']\n",
            "Q4 - Cleaned Text with Substitutions:\n",
            "I love exploring the intersection of technology and human behavior. \n",
            "Artificial Intelligence is transforming industries, especially healthcare and education. \n",
            "My fascination lies in how machines learn to think and make decisions. \n",
            "Books and podcasts keep me updated on cutting-edge advancements. \n",
            "I'm particularly intrigued by ethical implications of autonomous systems.\n"
          ]
        }
      ]
    }
  ]
}